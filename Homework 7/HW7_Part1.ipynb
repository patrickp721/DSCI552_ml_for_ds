{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpDTKQnrH3Jq"
   },
   "source": [
    "# DSCI552: HW7\n",
    "#Part 1: Generative Models for Text \n",
    "\n",
    "\n",
    "***Name: Cheng Peng***\n",
    "\n",
    "\n",
    "***USC ID: 6898-9638-37***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1JgeJIvKMwH"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import LSTM \n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import string\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XBoC-po6vAU"
   },
   "source": [
    "# A):  \n",
    "\n",
    "\n",
    "In this problem, we are trying to build a generative model to mimic the writing style of prominent British Mathematician, Philosopher, prolific writer, and political activist, Bertrand Russell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTA1z_Dn6z7E"
   },
   "source": [
    "# B): Download the data and prepare the corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaPkg1ytIlYQ"
   },
   "source": [
    "***Comment:***I am using google collab with 12.72 GB of RAM. I tired to include all books in the corpus but it used up all the memory. I decided to only use the first 4 books as my corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3928,
     "status": "ok",
     "timestamp": 1605884021205,
     "user": {
      "displayName": "Patrick Peng",
      "photoUrl": "",
      "userId": "09032932124181776438"
     },
     "user_tz": 300
    },
    "id": "1igmfp6L3hyG",
    "outputId": "8ff44a0a-db6d-4901-cbfe-402af438243d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write MLOE.txt to the corpus\n",
      "write OKEWFSMP.txt to the corpus\n",
      "write TPP.txt to the corpus\n",
      "write TAM.txt to the corpus\n"
     ]
    }
   ],
   "source": [
    "#base_directory = \"/content/drive/MyDrive/DSCI552/Homework 7 Data/Book Files/books\"\n",
    "base_directory = \"../data/Book Files/books\"\n",
    "files = os.listdir(base_directory)\n",
    "corpus = open(\"corpus.txt\", \"w\")\n",
    "for file in files:\n",
    "    print(\"write {} to the corpus\".format(file))\n",
    "    with open(base_directory+\"/\"+file, 'r', encoding='ascii', errors='ignore') as datastream:\n",
    "        for line in datastream:\n",
    "            corpus.write(line)\n",
    "    \n",
    "corpus.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WH9Ige5-Bvd"
   },
   "source": [
    "# C): LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jnCnTy--Mvo"
   },
   "source": [
    "I): Concatenate your text files to create a corpus of Russellâ€™s writings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1605884025480,
     "user": {
      "displayName": "Patrick Peng",
      "photoUrl": "",
      "userId": "09032932124181776438"
     },
     "user_tz": 300
    },
    "id": "7sS61Cwe7LfN",
    "outputId": "4a7b26f1-c4cc-4ff7-b7a3-95e9d248489e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1576925 characters in the corpus.\n"
     ]
    }
   ],
   "source": [
    "# load the book \n",
    "corpus = open(\"corpus.txt\", \"r\", encoding = \"ascii\").read()\n",
    "print(\"There are {} characters in the corpus.\".format(len(corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S807SSId-QhI"
   },
   "source": [
    "II): Convert to character level representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGOXyczZNx3Q"
   },
   "outputs": [],
   "source": [
    "# find the unique characters in the corpus \n",
    "# convert to all lower cases and remove the punctuations \n",
    "punctuations = string.punctuation\n",
    "corpus = corpus.lower()\n",
    "corpus = corpus.translate(str.maketrans('', '', punctuations))\n",
    "chars = sorted(list(set(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1605884031338,
     "user": {
      "displayName": "Patrick Peng",
      "photoUrl": "",
      "userId": "09032932124181776438"
     },
     "user_tz": 300
    },
    "id": "HiffTQzqQYxv",
    "outputId": "d9c4cb21-57ea-4ff0-f6f9-626d00a1f77b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 38 unique characters in the corpus.\n",
      "['\\n', ' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} unique characters in the corpus.\".format(len(chars)))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ft9WWfVaRDRt"
   },
   "outputs": [],
   "source": [
    "# creat mapping from character to integer and integer to character \n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNnHkfoN-Yw3"
   },
   "source": [
    "III): Choose a window size W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kO-qj3lw-dwY"
   },
   "source": [
    "IV): Construct input and output using the sliding window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RP57_o62SLvm"
   },
   "outputs": [],
   "source": [
    "# Set the sliding window size = W\n",
    "# i want the length of each input sequence to be 100, thus set W to 101 \n",
    "W = 101 \n",
    "inputs, outputs = [], []\n",
    "for i in range(0, len(corpus)-W-1):\n",
    "    x = corpus[i:i+W-1]\n",
    "    y = corpus[i+W-1]\n",
    "    # convert the characters to integer \n",
    "    inputs.append([char_to_int[c] for c in x])\n",
    "    outputs.append(char_to_int[y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11106,
     "status": "ok",
     "timestamp": 1605884057414,
     "user": {
      "displayName": "Patrick Peng",
      "photoUrl": "",
      "userId": "09032932124181776438"
     },
     "user_tz": 300
    },
    "id": "I1nIe1azUP3t",
    "outputId": "6fb08eb1-3495-4c6f-f8db-926dd9b406d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1532602"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10683,
     "status": "ok",
     "timestamp": 1605884057414,
     "user": {
      "displayName": "Patrick Peng",
      "photoUrl": "",
      "userId": "09032932124181776438"
     },
     "user_tz": 300
    },
    "id": "4MllE4mhUYza",
    "outputId": "c76db7ac-e266-4b63-c119-a9b1389407f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1532602"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10244,
     "status": "ok",
     "timestamp": 1605884057415,
     "user": {
      "displayName": "Patrick Peng",
      "photoUrl": "",
      "userId": "09032932124181776438"
     },
     "user_tz": 300
    },
    "id": "uhzNAAHXUdZN",
    "outputId": "993da2a1-20c3-44e8-eac5-58c4e92dcb56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1532602 input sequences and each sequence is of length 100\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} input sequences and each sequence is of length {}\".format(len(inputs), len(inputs[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQBRU-ID-pOd"
   },
   "source": [
    "V): One-hot encoding on the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4E1odt-UtfQ"
   },
   "outputs": [],
   "source": [
    "# use the to_categorical function to convert y to one hot endcoding \n",
    "outputs = to_categorical(outputs, num_classes=len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1605884108144,
     "user": {
      "displayName": "Patrick Peng",
      "photoUrl": "",
      "userId": "09032932124181776438"
     },
     "user_tz": 300
    },
    "id": "-YYB7rR_DE5Z",
    "outputId": "370dc91d-cce6-4bd2-9781-06fa4f8c6e6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output has been converted to a matrix of shape (1532602, 38), each output character is encoded using one-hot encoding. \n"
     ]
    }
   ],
   "source": [
    "print(\"The output has been converted to a matrix of shape {}, each output character is encoded using one-hot encoding. \".format(outputs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1605884384322,
     "user": {
      "displayName": "Patrick Peng",
      "photoUrl": "",
      "userId": "09032932124181776438"
     },
     "user_tz": 300
    },
    "id": "yOGzLm51Ddrb",
    "outputId": "e3141ed4-a438-494a-cf44-5336d7a8784c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For instance, output character is h is encoded as \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"For instance, output character is {} is encoded as \\n{}\".format(int_to_char[np.where(outputs[5]==1)[0][0]], outputs[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_iCzQXDSEt-X"
   },
   "outputs": [],
   "source": [
    "inputs = np.array(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfYOj7NLLDbu"
   },
   "outputs": [],
   "source": [
    "# reshape the input dimension to be [num_samples, sequence_length, features]\n",
    "inputs = np.expand_dims(inputs, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XG_Jc-zZ-3xY"
   },
   "source": [
    "VI): Single hidden layer for the LSTM with N=256 memory units \n",
    "\n",
    "VII): Softmax output layer and cross entropy as the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "po6bQX2dM3qn"
   },
   "outputs": [],
   "source": [
    "# normalize inputs to be between [0, 1]\n",
    "inputs = inputs/float(len(chars)-1)\n",
    "X = inputs\n",
    "y = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1605886163237,
     "user": {
      "displayName": "Patrick Peng",
      "photoUrl": "",
      "userId": "09032932124181776438"
     },
     "user_tz": 300
    },
    "id": "lfnlxtUPRi_L",
    "outputId": "0dded33e-d6d7-4544-c0d5-7542738f1075"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1], X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zRa6hLmEUO3"
   },
   "outputs": [],
   "source": [
    "# define a model \n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1605886584293,
     "user": {
      "displayName": "Patrick Peng",
      "photoUrl": "",
      "userId": "09032932124181776438"
     },
     "user_tz": 300
    },
    "id": "fh-PyWGuSYXD",
    "outputId": "cdd24d98-a9fd-484c-c6b5-55b7b500ade6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 38)                9766      \n",
      "=================================================================\n",
      "Total params: 273,958\n",
      "Trainable params: 273,958\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1605886263460,
     "user": {
      "displayName": "Patrick Peng",
      "photoUrl": "",
      "userId": "09032932124181776438"
     },
     "user_tz": 300
    },
    "id": "yCLJJW5xR53u",
    "outputId": "f94d551e-fcd9-4fda-cb25-d6ed862396e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 100, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1605886277304,
     "user": {
      "displayName": "Patrick Peng",
      "photoUrl": "",
      "userId": "09032932124181776438"
     },
     "user_tz": 300
    },
    "id": "gDiuAnhZR8q1",
    "outputId": "2e8244f8-0418-482e-de30-3120f94aa123"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 100, 38)"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtAbucD8_X-K"
   },
   "source": [
    "VIII): Use the entire dataset to train the model \n",
    "\n",
    "IX): Training \n",
    "\n",
    "X): Use model check-point to keep the network weight, find the best set of weight in terms of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M54Z3YLF9uLA"
   },
   "outputs": [],
   "source": [
    "# define the chackpoints \n",
    "model_file = \"model_{epoch:02d}_{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(model_file, monitor='loss', verbose=1, save_best_only=True)\n",
    "callback = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4643382,
     "status": "ok",
     "timestamp": 1605891270801,
     "user": {
      "displayName": "Patrick Peng",
      "photoUrl": "",
      "userId": "09032932124181776438"
     },
     "user_tz": 300
    },
    "id": "U1ojsUy5QW_k",
    "outputId": "e22d71b6-8b2b-467f-8e7a-270c85c19bbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "11972/11974 [============================>.] - ETA: 0s - loss: 2.4856\n",
      "Epoch 00001: loss improved from inf to 2.48559, saving model to model_01_2.4856.hdf5\n",
      "11974/11974 [==============================] - 154s 13ms/step - loss: 2.4856\n",
      "Epoch 2/30\n",
      "11973/11974 [============================>.] - ETA: 0s - loss: 2.0814\n",
      "Epoch 00002: loss improved from 2.48559 to 2.08144, saving model to model_02_2.0814.hdf5\n",
      "11974/11974 [==============================] - 155s 13ms/step - loss: 2.0814\n",
      "Epoch 3/30\n",
      "11974/11974 [==============================] - ETA: 0s - loss: 1.9020\n",
      "Epoch 00003: loss improved from 2.08144 to 1.90201, saving model to model_03_1.9020.hdf5\n",
      "11974/11974 [==============================] - 155s 13ms/step - loss: 1.9020\n",
      "Epoch 4/30\n",
      "11973/11974 [============================>.] - ETA: 0s - loss: 1.7973\n",
      "Epoch 00004: loss improved from 1.90201 to 1.79726, saving model to model_04_1.7973.hdf5\n",
      "11974/11974 [==============================] - 155s 13ms/step - loss: 1.7973\n",
      "Epoch 5/30\n",
      "11974/11974 [==============================] - ETA: 0s - loss: 1.7251\n",
      "Epoch 00005: loss improved from 1.79726 to 1.72508, saving model to model_05_1.7251.hdf5\n",
      "11974/11974 [==============================] - 155s 13ms/step - loss: 1.7251\n",
      "Epoch 6/30\n",
      "11972/11974 [============================>.] - ETA: 0s - loss: 1.6707\n",
      "Epoch 00006: loss improved from 1.72508 to 1.67064, saving model to model_06_1.6706.hdf5\n",
      "11974/11974 [==============================] - 155s 13ms/step - loss: 1.6706\n",
      "Epoch 7/30\n",
      "11970/11974 [============================>.] - ETA: 0s - loss: 1.6281\n",
      "Epoch 00007: loss improved from 1.67064 to 1.62807, saving model to model_07_1.6281.hdf5\n",
      "11974/11974 [==============================] - 154s 13ms/step - loss: 1.6281\n",
      "Epoch 8/30\n",
      "11970/11974 [============================>.] - ETA: 0s - loss: 1.5918\n",
      "Epoch 00008: loss improved from 1.62807 to 1.59183, saving model to model_08_1.5918.hdf5\n",
      "11974/11974 [==============================] - 155s 13ms/step - loss: 1.5918\n",
      "Epoch 9/30\n",
      "11973/11974 [============================>.] - ETA: 0s - loss: 1.5670\n",
      "Epoch 00009: loss improved from 1.59183 to 1.56704, saving model to model_09_1.5670.hdf5\n",
      "11974/11974 [==============================] - 154s 13ms/step - loss: 1.5670\n",
      "Epoch 10/30\n",
      "11971/11974 [============================>.] - ETA: 0s - loss: 1.5446\n",
      "Epoch 00010: loss improved from 1.56704 to 1.54457, saving model to model_10_1.5446.hdf5\n",
      "11974/11974 [==============================] - 156s 13ms/step - loss: 1.5446\n",
      "Epoch 11/30\n",
      "11973/11974 [============================>.] - ETA: 0s - loss: 1.5231\n",
      "Epoch 00011: loss improved from 1.54457 to 1.52306, saving model to model_11_1.5231.hdf5\n",
      "11974/11974 [==============================] - 154s 13ms/step - loss: 1.5231\n",
      "Epoch 12/30\n",
      "11973/11974 [============================>.] - ETA: 0s - loss: 1.5042\n",
      "Epoch 00012: loss improved from 1.52306 to 1.50424, saving model to model_12_1.5042.hdf5\n",
      "11974/11974 [==============================] - 155s 13ms/step - loss: 1.5042\n",
      "Epoch 13/30\n",
      "11971/11974 [============================>.] - ETA: 0s - loss: 1.4873\n",
      "Epoch 00013: loss improved from 1.50424 to 1.48733, saving model to model_13_1.4873.hdf5\n",
      "11974/11974 [==============================] - 155s 13ms/step - loss: 1.4873\n",
      "Epoch 14/30\n",
      "11974/11974 [==============================] - ETA: 0s - loss: 1.4721\n",
      "Epoch 00014: loss improved from 1.48733 to 1.47215, saving model to model_14_1.4721.hdf5\n",
      "11974/11974 [==============================] - 155s 13ms/step - loss: 1.4721\n",
      "Epoch 15/30\n",
      "11973/11974 [============================>.] - ETA: 0s - loss: 1.4580\n",
      "Epoch 00015: loss improved from 1.47215 to 1.45802, saving model to model_15_1.4580.hdf5\n",
      "11974/11974 [==============================] - 154s 13ms/step - loss: 1.4580\n",
      "Epoch 16/30\n",
      "11972/11974 [============================>.] - ETA: 0s - loss: 1.4453\n",
      "Epoch 00016: loss improved from 1.45802 to 1.44534, saving model to model_16_1.4453.hdf5\n",
      "11974/11974 [==============================] - 155s 13ms/step - loss: 1.4453\n",
      "Epoch 17/30\n",
      "11974/11974 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 00017: loss improved from 1.44534 to 1.43420, saving model to model_17_1.4342.hdf5\n",
      "11974/11974 [==============================] - 155s 13ms/step - loss: 1.4342\n",
      "Epoch 18/30\n",
      "11972/11974 [============================>.] - ETA: 0s - loss: 1.4237\n",
      "Epoch 00018: loss improved from 1.43420 to 1.42373, saving model to model_18_1.4237.hdf5\n",
      "11974/11974 [==============================] - 155s 13ms/step - loss: 1.4237\n",
      "Epoch 19/30\n",
      "11973/11974 [============================>.] - ETA: 0s - loss: 1.4142\n",
      "Epoch 00019: loss improved from 1.42373 to 1.41421, saving model to model_19_1.4142.hdf5\n",
      "11974/11974 [==============================] - 155s 13ms/step - loss: 1.4142\n",
      "Epoch 20/30\n",
      "11972/11974 [============================>.] - ETA: 0s - loss: 1.4054\n",
      "Epoch 00020: loss improved from 1.41421 to 1.40543, saving model to model_20_1.4054.hdf5\n",
      "11974/11974 [==============================] - 155s 13ms/step - loss: 1.4054\n",
      "Epoch 21/30\n",
      "11971/11974 [============================>.] - ETA: 0s - loss: 1.3971\n",
      "Epoch 00021: loss improved from 1.40543 to 1.39714, saving model to model_21_1.3971.hdf5\n",
      "11974/11974 [==============================] - 155s 13ms/step - loss: 1.3971\n",
      "Epoch 22/30\n",
      "11974/11974 [==============================] - ETA: 0s - loss: 1.3895\n",
      "Epoch 00022: loss improved from 1.39714 to 1.38947, saving model to model_22_1.3895.hdf5\n",
      "11974/11974 [==============================] - 154s 13ms/step - loss: 1.3895\n",
      "Epoch 23/30\n",
      "11971/11974 [============================>.] - ETA: 0s - loss: 1.3824\n",
      "Epoch 00023: loss improved from 1.38947 to 1.38238, saving model to model_23_1.3824.hdf5\n",
      "11974/11974 [==============================] - 154s 13ms/step - loss: 1.3824\n",
      "Epoch 24/30\n",
      "11974/11974 [==============================] - ETA: 0s - loss: 1.3757\n",
      "Epoch 00024: loss improved from 1.38238 to 1.37568, saving model to model_24_1.3757.hdf5\n",
      "11974/11974 [==============================] - 154s 13ms/step - loss: 1.3757\n",
      "Epoch 25/30\n",
      "11973/11974 [============================>.] - ETA: 0s - loss: 1.3692\n",
      "Epoch 00025: loss improved from 1.37568 to 1.36924, saving model to model_25_1.3692.hdf5\n",
      "11974/11974 [==============================] - 154s 13ms/step - loss: 1.3692\n",
      "Epoch 26/30\n",
      "11970/11974 [============================>.] - ETA: 0s - loss: 1.3630\n",
      "Epoch 00026: loss improved from 1.36924 to 1.36308, saving model to model_26_1.3631.hdf5\n",
      "11974/11974 [==============================] - 153s 13ms/step - loss: 1.3631\n",
      "Epoch 27/30\n",
      "11971/11974 [============================>.] - ETA: 0s - loss: 1.3574\n",
      "Epoch 00027: loss improved from 1.36308 to 1.35739, saving model to model_27_1.3574.hdf5\n",
      "11974/11974 [==============================] - 154s 13ms/step - loss: 1.3574\n",
      "Epoch 28/30\n",
      "11972/11974 [============================>.] - ETA: 0s - loss: 1.3520\n",
      "Epoch 00028: loss improved from 1.35739 to 1.35206, saving model to model_28_1.3521.hdf5\n",
      "11974/11974 [==============================] - 154s 13ms/step - loss: 1.3521\n",
      "Epoch 29/30\n",
      "11974/11974 [==============================] - ETA: 0s - loss: 1.3474\n",
      "Epoch 00029: loss improved from 1.35206 to 1.34739, saving model to model_29_1.3474.hdf5\n",
      "11974/11974 [==============================] - 154s 13ms/step - loss: 1.3474\n",
      "Epoch 30/30\n",
      "11973/11974 [============================>.] - ETA: 0s - loss: 1.3428\n",
      "Epoch 00030: loss improved from 1.34739 to 1.34281, saving model to model_30_1.3428.hdf5\n",
      "11974/11974 [==============================] - 154s 13ms/step - loss: 1.3428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f18ac518860>"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=30, batch_size = 128, callbacks=callback, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_C74I4g2_40u"
   },
   "source": [
    "XI): Use the netowrk with the best weights to generate 1000 characters. \n",
    "\n",
    "Input: There are those who take mental phenomena naively, just as they\n",
    "would physical phenomena. This school of psychologists tends not to\n",
    "emphasize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsP-IpQ5mgZm"
   },
   "outputs": [],
   "source": [
    "# use the best model to generate text \n",
    "best_model = \"/content/model_30_1.3428.hdf5\"\n",
    "model.load_weights(best_model)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5l4nElDnd1L"
   },
   "outputs": [],
   "source": [
    "# encode the sample given \n",
    "input_text = \"There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.\".lower()\n",
    "input_text = input_text.translate(str.maketrans('', '', punctuations))\n",
    "input_sequence = [char_to_int[c] for c in input_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3rZjN8Bonic"
   },
   "outputs": [],
   "source": [
    "# generate 1000 characters \n",
    "for i in range(1000):\n",
    "    X = np.reshape(input_sequence, (1, len(input_sequence), 1))\n",
    "    X = X/float(len(chars)-1)\n",
    "    predict = model.predict(X, verbose=0)\n",
    "    index = np.argmax(predict)\n",
    "    char_predict = int_to_char[index]\n",
    "    input_text+=char_predict\n",
    "    input_sequence.append(index)\n",
    "    input_sequence = input_sequence[1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "executionInfo": {
     "elapsed": 370,
     "status": "ok",
     "timestamp": 1605898422056,
     "user": {
      "displayName": "Patrick Peng",
      "photoUrl": "",
      "userId": "09032932124181776438"
     },
     "user_tz": 300
    },
    "id": "01bx31-CvoeT",
    "outputId": "4d4b69f4-eb4f-4631-fc85-da72975035f5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'there are those who take mental phenomena naively just as they would physical phenomena this school of psychologists tends not to emphasize the objective of the perspective shat in the sense of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation of the sensedata which is the semsation '"
      ]
     },
     "execution_count": 126,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TX3X0QhexJnC"
   },
   "source": [
    "***Comment:***\n",
    "\n",
    "\n",
    "* The model is able to generate sentences that confroms to the line format of the original text \n",
    "* The generated characters are grouped into \"words\", some words are correctlt spelled which means the model learns some spelling.   \n",
    "* The model keep generating the same sentence (\"which is the semsation of the sensedata\"), the reason can be lack of varity in the training samples, and small sample size.  \n",
    "*   Since all the punctuation are removed in the traning set, the mode cannot generate any punctuation marks. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rhIa8xI0tMX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQ4HlE7n05Bn"
   },
   "source": [
    "XII): More training with a deeper model \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulNOXoN00_Ca"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Mc97yL098bW"
   },
   "source": [
    "# References:\n",
    "1. https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NA7XL9zH9-fP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO3cObfU4FVUFWv6U79gyDc",
   "collapsed_sections": [],
   "mount_file_id": "1lOS9Ug84aaa3FIFVFGUBz1GX_cEmlKgt",
   "name": "HW7_Part1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
